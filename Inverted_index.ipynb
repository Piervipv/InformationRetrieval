{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this section we build the inverted_index indexing the previously crawled documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide directory_path where the documents of the collection are contained. \n",
      " To select the default option \"./cranfieldDocs\" press d d\n",
      "./documents2\n"
     ]
    }
   ],
   "source": [
    "directory_input = input('Please provide directory_path where the documents of the collection are contained. \\n To select the default option | ./documents2 | press d ')\n",
    "if (directory_input.lower() == 'd'):\n",
    "    \n",
    "    directory_path ='./documents2' #TODO: read from user inmput\n",
    "else:\n",
    "    directory_path=directory_input\n",
    "    \n",
    "documents=os.listdir(directory_path)\n",
    "print(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide stop_words_path txt file . \n",
      " To select the default option \"./stopwords.tx\" press d d\n",
      "./stopwords.txt\n"
     ]
    }
   ],
   "source": [
    "tokenizer=RegexpTokenizer('[a-z]+[-]?[0-9a-z]*')\n",
    "stemmer=PorterStemmer()\n",
    "#retrive the list of stopwords\n",
    "stop_words_input = input('Please provide stop_words_path txt file . \\n To select the default option \"./stopwords.tx\" press d ')\n",
    "if (stop_words_input.lower() == 'd'):\n",
    "    \n",
    "    stopwords_path =\"./stopwords.txt\"  \n",
    "else:\n",
    "    stopwords_path =stop_words_input\n",
    "print(stopwords_path)\n",
    "stopwords = open(stopwords_path).read()\n",
    "stopwords_list=stopwords.split()\n",
    "for stop in stopwords_list:\n",
    "    temp = stemmer.stem(stop)\n",
    "    if not (temp in stopwords_list):\n",
    "    \n",
    "       stopwords_list.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class token_class:\n",
    "    def __init__(self, count= 0, idf=0,containing_documents= {}):\n",
    "        self.idf=idf #number of containing docs\n",
    "        self.containing_documents=containing_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punt_and_number(string):\n",
    "    remove_list=['0','1','2','3','4','5','6','7','8','9' ]\n",
    "    #remove_list=[]\n",
    "    string2=''\n",
    "    for i in range(0,len(string)):\n",
    "        if not(string[i] in remove_list):\n",
    "            string2=string2 +string[i]\n",
    "    return string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to perform preprocessing: remove puntaction,tokenize,stemming, stopwords and words of 1,2 characters\n",
    "#return the list of tokens in the string/document\n",
    "def preprocess(string,stopwords_list ):\n",
    "   # re.findall\n",
    "    tokens=[]\n",
    "    tokenizer=RegexpTokenizer('([a-z]*[-]?[a-z]+[-]?[a-z]*|[0-9]+[-]?[a-z]+|[a-z]+[-]?[0-9]+)') #matches only alphabetical characters\n",
    "    tokens=tokenizer.tokenize(string.lower())\n",
    "    #remove_punt_and_number(string)#remove puntanction\n",
    "    stemmer=PorterStemmer()\n",
    "    stemmed_tokens=[]\n",
    "    for word in tokens:\n",
    "        token=remove_punt_and_number(word)\n",
    "        token=stemmer.stem(token);\n",
    "        token=token.strip(\"-\")\n",
    "        alpha=re.findall('[a-z]', token)\n",
    "        if not(token in stopwords_list or len(alpha)<3): #condition to eliminate stopwords\n",
    "            stemmed_tokens.append(token) #stemming\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the occurencies of a word:String in a document: List of tokens\n",
    "def occurencies(List, String):\n",
    "    count=0\n",
    "    for x in List:\n",
    "        if x==String:\n",
    "            count=count +1\n",
    "    return count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the occurencies of each token in each documents_dictionary element, if >0 add document_id : occurencies \n",
    "#returns count and hash Map containing_docs : freq\n",
    "def index_token(token, documents_dictionary):\n",
    "    containing_documents={}\n",
    "    count=0\n",
    "    idf=0\n",
    "    for i in range (0,len(documents_dictionary)): \n",
    "        doc_freq=occurencies(documents_dictionary[str(i)], token)\n",
    "        if doc_freq>0:\n",
    "            count += doc_freq\n",
    "            idf += 1\n",
    "            key=str(i)\n",
    "            value=doc_freq\n",
    "            containing_documents[key] = value\n",
    "    return count,idf, containing_documents\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "documents_dictionary[\"1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x =inverted_index[\"uic\"]\n",
    "x.containing_documents.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "weighted_norm(str(0), documents_dictionary, inverted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inverted_index[\"uic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the weighted norm of a document\n",
    "def weighted_norm(document_id, documents_dictionary, inverted_index):\n",
    "    tokens=documents_dictionary[document_id] #retrieve the tokens in the document\n",
    "    #print(tokens)\n",
    "    sum=0\n",
    "    for token in tokens:\n",
    "        term_freq=tf(token, document_id,inverted_index)\n",
    "        #print(term_freq)\n",
    "        idfe=idf(token, documents_dictionary, inverted_index)\n",
    "        weight=term_freq*idfe\n",
    "        sum += weight * weight\n",
    "    return sum**(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def query_norm(query_tokens, documents_dictionary, inverted_index):\n",
    "    #quertokens=documents_dictionary[document_id]\n",
    "    sum=0\n",
    "    term_freq=0\n",
    "    for token in set(query_tokens):\n",
    "        #print(term_freq)\n",
    "        term_freq=occurencies(query_tokens, token)\n",
    "        idfe=idf(token, documents_dictionary, inverted_index) #documents_dictionary is used to retrieve N\n",
    "        weight=term_freq*idfe\n",
    "        #print(weight) #debug\n",
    "        sum += weight * weight\n",
    "        #print(sum) #debug\n",
    "    return sum**(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def retrive_docs(query_tokens, documents_dictionary, inverted_index):\n",
    "    matching_docs=set()\n",
    "    token_list=list(inverted_index.keys())\n",
    "    for tokens in query_tokens:\n",
    "        #print(\"token:\")\n",
    "        #print(tokens)\n",
    "        if (tokens in token_list):\n",
    "            x=inverted_index[tokens]\n",
    "            #print(\"containing docs:\")\n",
    "            #print(list(x.containing_documents.keys()))\n",
    "            documents=list(x.containing_documents.keys())\n",
    "            #print(\"documents\")\n",
    "            #print(documents)\n",
    "            matching_docs.update(documents)\n",
    "    #print(matching_docs) #debug\n",
    "    return matching_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#query is a list of tokens, matching docs is a list of documents' IDs\n",
    "#for each token, incrementally compute cosine_similarity for each document\n",
    "#at the end, divide by document and query norm\n",
    "#returns a hash doc_id : similarity value\n",
    "def compute_similarity(query, matching_docs, documents_dictionary, inverted_index, documents_weighted_norm):\n",
    "    similarity={}\n",
    "    score=0\n",
    "    #initialize similarity dictionary\n",
    "    for doc in matching_docs:\n",
    "        key=doc\n",
    "        value=int(0)\n",
    "        similarity[key]=value\n",
    "    #compute the accumulated similarity\n",
    "    for token in query:\n",
    "        for doc in matching_docs:\n",
    "            key=doc\n",
    "            score=tf(token,doc,inverted_index)*idf(token,documents_dictionary,inverted_index)**2\n",
    "            similarity[key] += score #increment the similarity\n",
    "    #divide by document and query's norm\n",
    "    for doc in matching_docs:\n",
    "        key=doc\n",
    "        divider=documents_weighted_norm[key]\n",
    "        #print(divider)\n",
    "        divider= divider * query_norm(query, documents_dictionary, inverted_index)\n",
    "        #print(divider)\n",
    "        similarity[key] = similarity[key] / divider\n",
    "    #print (\"similarity_dict\")\n",
    "    #print (similarity) #debug\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ranks the documents\n",
    "#returns an ordered list of tuples\n",
    "def rank_docs(similarity_dictionary):\n",
    "    from operator import itemgetter\n",
    "    sorted_docs = sorted(similarity_dictionary.items(), key=itemgetter(1), reverse=True)\n",
    "    #debug\n",
    "    #print(\"sorted docs:\")\n",
    "    #print(sorted_docs)\n",
    "    #print(type(sorted_docs))\n",
    "    #print(sorted_docs[1])\n",
    "    return sorted_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def retrive_ranked_docs(query,documents_dictionary,inverted_index,documents_weighted_norm,stopwords_list):\n",
    "    query_tokens=preprocess(query,stopwords_list)\n",
    "    #print(\"query tokens\")\n",
    "    #print(query_tokens)\n",
    "    #query_weighted_norm=query_norm(query_tokens, documents_dictionary, inverted_index)\n",
    "    matching_documents= retrive_docs(query_tokens, documents_dictionary, inverted_index)\n",
    "    #print('matching docs')\n",
    "    #print(len(matching_documents))\n",
    "    #print(matching_documents)\n",
    "    similarity_dictionary=compute_similarity(query_tokens, matching_documents, documents_dictionary, inverted_index, documents_weighted_norm)\n",
    "    #print(ranking)\n",
    "    ranking=rank_docs(similarity_dictionary)\n",
    "    #print(\"ranking\")\n",
    "    #print (ranking)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def retrieve_top_N_docs(ranked_docs, N):\n",
    "    return ranked_docs[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id_extractor=RegexpTokenizer('[0-9]+') #to extract the document ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the hash map documentID: list of contained tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building documents dictionary Id: list of contained tokens...\n",
      "63877\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "documents_dictionary={} #document id : list of tokens\n",
    "all_tokens_in_the_collection=set([]) #needed to extract the set and cycle over the tokens\n",
    "#for each document extract ID(key) and preprocess the text (value)\n",
    "print('Building documents dictionary Id: list of contained tokens...')\n",
    "for document in documents:\n",
    "    #temp_doc=open(directory_path +'/'+ document).read().decode('utf-8')\n",
    "    \n",
    "    with io.open(directory_path +'/'+ document, 'r', encoding=\"utf-8\") as file:\n",
    "        temp_doc=file.read()\n",
    "\n",
    "    #extract title and text from the html file\n",
    "    temp = document.split(\".\")\n",
    "    key=temp[0]\n",
    "    \n",
    "    text = temp_doc.replace('\\n',' ')\n",
    "    value=text.replace('/',' ')\n",
    "    \n",
    "    #value= title + \" \" + text + \" \"\n",
    "    #preprocess the document\n",
    "    #print(type(value))\n",
    "    value=preprocess(value, stopwords_list) #value changes from String to list of preprocessed tokens\n",
    "    #print(type(value))\n",
    "    value2=set(value)\n",
    "    all_tokens_in_the_collection.update(value2) #update the collection of tokens\n",
    "    documents_dictionary[key] = value #add in the map ID: list of tokens\n",
    "print(len(all_tokens_in_the_collection))\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['catalog',\n",
       " 'uic',\n",
       " 'search',\n",
       " 'phar',\n",
       " 'search',\n",
       " 'result',\n",
       " 'univers',\n",
       " 'illinoi',\n",
       " 'chicago',\n",
       " 'skip',\n",
       " 'content',\n",
       " 'index',\n",
       " 'catalog',\n",
       " 'home',\n",
       " 'institut',\n",
       " 'home',\n",
       " 'academ',\n",
       " 'catalog',\n",
       " 'search',\n",
       " 'catalog',\n",
       " 'undergradu',\n",
       " 'catalog',\n",
       " 'graduat',\n",
       " 'catalog',\n",
       " 'print',\n",
       " 'download',\n",
       " 'option',\n",
       " 'uic',\n",
       " 'home',\n",
       " 'univers',\n",
       " 'illinoi',\n",
       " 'chicago',\n",
       " 'catalog',\n",
       " 'navig',\n",
       " 'catalog',\n",
       " 'navig',\n",
       " 'undergradu',\n",
       " 'catalog',\n",
       " 'graduat',\n",
       " 'catalog',\n",
       " 'search',\n",
       " 'result',\n",
       " 'search',\n",
       " 'result',\n",
       " 'descript',\n",
       " 'home',\n",
       " 'search',\n",
       " 'result',\n",
       " 'search',\n",
       " 'result',\n",
       " 'search',\n",
       " 'result',\n",
       " 'phar',\n",
       " 'introduct',\n",
       " 'drug',\n",
       " 'inform',\n",
       " 'hour',\n",
       " 'content',\n",
       " 'focu',\n",
       " 'ompar',\n",
       " 'contrast',\n",
       " 'primari',\n",
       " 'secondari',\n",
       " 'tertiari',\n",
       " 'resourc',\n",
       " 'includ',\n",
       " 'trustworthi',\n",
       " 'student',\n",
       " 'gain',\n",
       " 'skill',\n",
       " 'conduct',\n",
       " 'systemat',\n",
       " 'search',\n",
       " 'extract',\n",
       " 'inform',\n",
       " 'sourc',\n",
       " 'univers',\n",
       " 'illinoi',\n",
       " 'chicago',\n",
       " 'appli',\n",
       " 'onlin',\n",
       " 'contact',\n",
       " 'admiss',\n",
       " 'privaci',\n",
       " 'statement',\n",
       " 'problem',\n",
       " 'access',\n",
       " 'site',\n",
       " 'contact',\n",
       " 'webmast',\n",
       " 'board',\n",
       " 'truste',\n",
       " 'univers',\n",
       " 'illinoi',\n",
       " 'back',\n",
       " 'top',\n",
       " 'print',\n",
       " 'option',\n",
       " 'send',\n",
       " 'page',\n",
       " 'printer',\n",
       " 'print',\n",
       " 'page',\n",
       " 'download',\n",
       " 'undergradu',\n",
       " 'catalog',\n",
       " 'page',\n",
       " 'campu',\n",
       " 'catalog',\n",
       " 'cancel']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_dictionary['3199']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(all_tokens_in_the_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the collection there are 63877 distinct preprocessed tokens.\n"
     ]
    }
   ],
   "source": [
    "distinct_tokens_collection=set(all_tokens_in_the_collection)\n",
    "x=len(distinct_tokens_collection)\n",
    "#print(type(x))\n",
    "#print(x)\n",
    "print('In the collection there are ' + str(x) + ' distinct preprocessed tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the collections there are 3200 documents.\n"
     ]
    }
   ],
   "source": [
    "print('In the collections there are ' + str(len(documents_dictionary)) + ' documents.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the inverted index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building inverted index...\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "28000\n",
      "28500\n",
      "29000\n",
      "29500\n",
      "30000\n",
      "30500\n",
      "31000\n",
      "31500\n",
      "32000\n",
      "32500\n",
      "33000\n",
      "33500\n",
      "34000\n",
      "34500\n",
      "35000\n",
      "35500\n",
      "36000\n",
      "36500\n",
      "37000\n",
      "37500\n",
      "38000\n",
      "38500\n",
      "39000\n",
      "39500\n",
      "40000\n",
      "40500\n",
      "41000\n",
      "41500\n",
      "42000\n",
      "42500\n",
      "43000\n",
      "43500\n",
      "44000\n",
      "44500\n",
      "45000\n",
      "45500\n",
      "46000\n",
      "46500\n",
      "47000\n",
      "47500\n",
      "48000\n",
      "48500\n",
      "49000\n",
      "49500\n",
      "50000\n",
      "50500\n",
      "51000\n",
      "51500\n",
      "52000\n",
      "52500\n",
      "53000\n",
      "53500\n",
      "54000\n",
      "54500\n",
      "55000\n",
      "55500\n",
      "56000\n",
      "56500\n",
      "57000\n",
      "57500\n",
      "58000\n",
      "58500\n",
      "59000\n",
      "59500\n",
      "60000\n",
      "60500\n",
      "61000\n",
      "61500\n",
      "62000\n",
      "62500\n",
      "63000\n",
      "63500\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "#inverted_index is an hash map\n",
    "#key: token , value: token_class containing tf, idf and another hash map (containing document id : frequencies )\n",
    "import io\n",
    "inverted_index={}\n",
    "print(\"Building inverted index...\")\n",
    "i = 0\n",
    "with io.open(\"./inverted_index.txt\", 'w', encoding=\"utf-8\") as file:\n",
    "    \n",
    "    for token in distinct_tokens_collection:\n",
    "        tf, idf, containing_documents = index_token(token, documents_dictionary)\n",
    "        key=token\n",
    "        #print(token)\n",
    "        \n",
    "        #write to file to be parsed later just before query time\n",
    "        file.write(key + \"<key|idf>\" + str(idf) + \"<idf|cont_docs>\" + str(containing_documents) + \"</element>\")\n",
    "        #file.write(\"<idf>\" + str(idf) + \"</idf>\")\n",
    "        value=token_class(tf, idf, containing_documents)\n",
    "        inverted_index[key]=value\n",
    "        i +=1\n",
    "        if ((i % 500) == 0):\n",
    "            print(str(i))\n",
    "        ##just to stop it before\n",
    "        #if (i==10):\n",
    "              # break\n",
    "        \n",
    "\n",
    "print('done.')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "x=inverted_index[\"function\"]\n",
    "print(list(x.containing_documents.keys()))\n",
    "print(len(list(x.containing_documents.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Let's write the inverted index into file\n",
    "import io\n",
    "with io.open(\"inverted_index.txt\", 'w', encoding=\"utf-8\") as file:\n",
    "    for key in inverted_index.keys():\n",
    "                file.write(\"<key>\" + key + </key>)\n",
    "                file.write(\"\\n\")\n",
    "                file.write(str(inverted_index[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "#x=list(inverted_index.keys())\n",
    "#print(type(x))\n",
    "#type(x[1])\n",
    "#print(x.containing_documents.keys())\n",
    "#print(inverted_index['investig'].idf)\n",
    "#print(idf(\"investig\", documents_dictionary, inverted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "#tf(\"investig\", '1' ,inverted_index)\n",
    "#tf('order', '14',inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "#weighted_norm('1', documents_dictionary, inverted_index) #should be 99...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a dictionary doc_id : doc_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the idf of the token\n",
    "def idf(token, documents_dictionary, inverted_index):\n",
    "    N=len(documents_dictionary)\n",
    "    token_list=list(inverted_index.keys())\n",
    "    if not(token in token_list):\n",
    "        return 0\n",
    "    df=inverted_index[token].idf #number of containing docs\n",
    "    n=(N//df)\n",
    "    idf=math.log(n,2)\n",
    "    return idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(token, document_id ,inverted_index):\n",
    "    token_list=list(inverted_index.keys())\n",
    "    if not(token in token_list):\n",
    "        return 0\n",
    "    x=inverted_index[token]\n",
    "    #print(document_id)\n",
    "    keys=list(x.containing_documents.keys())\n",
    "    if document_id in keys:\n",
    "        tf=x.containing_documents[document_id] \n",
    "    else:\n",
    "        tf=0\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uic',\n",
       " 'uic',\n",
       " 'comput',\n",
       " 'scienc',\n",
       " 'univers',\n",
       " 'illinoi',\n",
       " 'chicago',\n",
       " 'colleg',\n",
       " 'engin',\n",
       " 'make',\n",
       " 'gift',\n",
       " 'admiss',\n",
       " 'undergradu',\n",
       " 'admissionsundergradu',\n",
       " 'open',\n",
       " 'housegradu',\n",
       " 'admissionsacadem',\n",
       " 'undergradu',\n",
       " 'programsgradu',\n",
       " 'programsjoint',\n",
       " 'degreec',\n",
       " 'minorcoursesscholarshipsinternship',\n",
       " 'jobsresearch',\n",
       " 'research',\n",
       " 'labsbi',\n",
       " 'research',\n",
       " 'areasumm',\n",
       " 'reu',\n",
       " 'speopl',\n",
       " 'facultyadjunctsemeritu',\n",
       " 'facultystaffcommitteesabout',\n",
       " 'departmentdistinguish',\n",
       " 'lectur',\n",
       " 'seriesrec',\n",
       " 'newsabet',\n",
       " 'accreditationemploymentg',\n",
       " 'departmentcontact',\n",
       " 'admissionsundergradu',\n",
       " 'admissionsundergradu',\n",
       " 'open',\n",
       " 'housegradu',\n",
       " 'admissionsacademicsundergradu',\n",
       " 'programsgradu',\n",
       " 'programsjoint',\n",
       " 'degreec',\n",
       " 'minorcoursesscholarshipsinternship',\n",
       " 'jobsresearchresearch',\n",
       " 'labsbi',\n",
       " 'research',\n",
       " 'areasumm',\n",
       " 'reu',\n",
       " 'speoplefacultyadjunctsemeritu',\n",
       " 'facultystaffcommitteesabout',\n",
       " 'usour',\n",
       " 'departmentdistinguish',\n",
       " 'lectur',\n",
       " 'seriesrec',\n",
       " 'newsabet',\n",
       " 'accreditationemploymentg',\n",
       " 'departmentcontact',\n",
       " 'uic',\n",
       " 'comput',\n",
       " 'sciencehom',\n",
       " 'reap',\n",
       " 'benefit',\n",
       " 'internship',\n",
       " 'experi',\n",
       " 'full-tim',\n",
       " 'faculti',\n",
       " 'member',\n",
       " 'join',\n",
       " 'comput',\n",
       " 'scienc',\n",
       " 'depart',\n",
       " 'fall',\n",
       " 'comput',\n",
       " 'scienc',\n",
       " 'student',\n",
       " 'organ',\n",
       " 'recruit',\n",
       " 'member',\n",
       " 'involv',\n",
       " 'fair',\n",
       " 'learn',\n",
       " 'young',\n",
       " 'women',\n",
       " 'girl',\n",
       " 'code',\n",
       " 'uic',\n",
       " 'join',\n",
       " 'grow',\n",
       " 'uic',\n",
       " 'comput',\n",
       " 'scienc',\n",
       " 'faculti',\n",
       " 'view',\n",
       " 'open',\n",
       " 'positionsrec',\n",
       " 'news',\n",
       " 'recent',\n",
       " 'honor',\n",
       " 'recent',\n",
       " 'grant',\n",
       " 'award',\n",
       " 'uic',\n",
       " 'comput',\n",
       " 'scienc',\n",
       " 'newsconnect',\n",
       " 'depart',\n",
       " 'comput',\n",
       " 'scienc',\n",
       " 'morgan',\n",
       " 'room',\n",
       " 'seo',\n",
       " 'chicago',\n",
       " 'info',\n",
       " 'uic',\n",
       " 'quick',\n",
       " 'linksundergradu',\n",
       " 'open',\n",
       " 'houseundergradu',\n",
       " 'admissionsgradu',\n",
       " 'admissionsscholarshipscontact',\n",
       " 'usintern',\n",
       " 'linksform',\n",
       " 'student',\n",
       " 'affairscommitteesmak',\n",
       " 'giftcookieset',\n",
       " 'univers',\n",
       " 'illinoi',\n",
       " 'chicago',\n",
       " 'colleg',\n",
       " 'engin',\n",
       " 'copyright',\n",
       " 'board',\n",
       " 'truste',\n",
       " 'univers',\n",
       " 'illinoi',\n",
       " 'privaci',\n",
       " 'statement']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_dictionary[\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing the norm of each document...\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "documents_weighted_norm={}\n",
    "print(\"computing the norm of each document...\")\n",
    "i = 0;\n",
    "for x in documents_dictionary:\n",
    "    key=x\n",
    "    value= weighted_norm(x, documents_dictionary, inverted_index)\n",
    "    documents_weighted_norm[key]=value\n",
    "    i +=1\n",
    "    if ((i % 50) == 0):\n",
    "        print(str(i))\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178.69743934332922"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_weighted_norm['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to file the documents_weighted_norm dictionary\n",
    "with io.open(\"weighted_norm_dictionaryfull.txt\", 'w', encoding=\"utf-8\") as doc:\n",
    "    i=1\n",
    "    for document in documents_weighted_norm.keys():\n",
    "        doc.write(document + \"<id|value>\" +str(documents_weighted_norm[document]) + \"<element>\")\n",
    "        i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
