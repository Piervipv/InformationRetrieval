{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: run again inverted index and weighted norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "import operator\n",
    "import math\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide stop_words_path txt file . \n",
      " To select the default option \"./stopwords.tx\" press d d\n",
      "./stopwords.txt\n"
     ]
    }
   ],
   "source": [
    "#load stopwordList\n",
    "#retrive the list of stopwords\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "stop_words_input = input('Please provide stop_words_path txt file . \\n To select the default option \"./stopwords.tx\" press d ')\n",
    "if (stop_words_input.lower() == 'd'):\n",
    "    \n",
    "    stopwords_path =\"./stopwords.txt\"  \n",
    "else:\n",
    "    stopwords_path =stop_words_input\n",
    "print(stopwords_path)\n",
    "stopwords = open(stopwords_path).read()\n",
    "stopwords_list=stopwords.split()\n",
    "for stop in stopwords_list:\n",
    "    temp = stemmer.stem(stop)\n",
    "    if not (temp in stopwords_list):\n",
    "    \n",
    "       stopwords_list.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We have to apply to the queries the same preprocess used during the indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove pintaction and numbers\n",
    "def remove_punt_and_number(string):\n",
    "    remove_list=['0','1','2','3','4','5','6','7','8','9' ]\n",
    "    #remove_list=[]\n",
    "    string2=''\n",
    "    for i in range(0,len(string)):\n",
    "        if not(string[i] in remove_list):\n",
    "            string2=string2 +string[i]\n",
    "    return string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to perform preprocessing: remove puntaction,tokenize,stemming, stopwords and words of 1,2 characters\n",
    "#return the list of tokens in the string/document\n",
    "def preprocess(string):\n",
    "    tokens=[]\n",
    "    #define regular expression for tokens\n",
    "    tokenizer=RegexpTokenizer('([a-z]*[-]?[a-z]+[-]?[a-z]*|[0-9]+[-]?[a-z]+|[a-z]+[-]?[0-9]+)') \n",
    "    tokens=tokenizer.tokenize(string.lower())\n",
    "    #remove_punt_and_number(string)#remove puntanction\n",
    "    stemmer=PorterStemmer()\n",
    "    stemmed_tokens=[]\n",
    "    for word in tokens:\n",
    "        token=remove_punt_and_number(word)\n",
    "        token=stemmer.stem(token);\n",
    "        token=token.strip(\"-\")\n",
    "        alpha=re.findall('[a-z]', token)\n",
    "        if not(token in stopwords_list or len(alpha)<3): #condition to eliminate stopwords\n",
    "            stemmed_tokens.append(token) #stemming\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query is a list of tokens,\n",
    "#matching docs is a list of documents' IDs\n",
    "\n",
    "#for each token, incrementally compute cosine_similarity for each document\n",
    "#at the end, divide by document and query norm\n",
    "#returns a hash doc_id : similarity value\n",
    "\n",
    "def compute_similarity(query, matching_docs, weight_dictionary):\n",
    "    similarity={}\n",
    "    score=0\n",
    "    #initialize similarity dictionary\n",
    "    for doc in matching_docs:\n",
    "        key=doc\n",
    "        value=int(0)\n",
    "        similarity[key]=value\n",
    "    #compute the accumulated similarity\n",
    "    for token in query:\n",
    "        weight = 1\n",
    "        \n",
    "        #weight_dictionary contains the weight for the token coming from query expansion\n",
    "        #the weight is the similarity of the expanded token w.r.t. the original token\n",
    "        if token in weight_dictionary.keys():\n",
    "            weight = weight_dictionary[token]\n",
    "            \n",
    "        #if the token is not in the weight_dictionary\n",
    "        #it means that is not an expanded token but a \n",
    "        #token that was present in the original query,\n",
    "        #so its weight is equal to 1\n",
    "        \n",
    "        for doc in matching_docs:\n",
    "            key=doc\n",
    "            score= weight * tf(token,doc)*idf(token)**2 \n",
    "            similarity[key] += score #increment the similarity\n",
    "            \n",
    "    #divide by document and query's norm\n",
    "    for doc in matching_docs:\n",
    "        key=doc\n",
    "        \n",
    "        #take the norm of the document\n",
    "        divider=weighted_norm_dictionary[key]\n",
    "        #####TODO:\n",
    "        #if (divider==0):\n",
    "            #divider+=1\n",
    "        #print(divider)\n",
    "        \n",
    "        #we don't need the qouery norm so the line below is commented\n",
    "        #divider= divider * query_norm(query)\n",
    "        #print(divider)\n",
    "        \n",
    "        similarity[key] = similarity[key] / divider\n",
    "        \n",
    "    #debug    \n",
    "    #print (\"similarity_dict\")\n",
    "    #print (similarity)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve only the documents that contains at least 1 token of the query\n",
    "def retrive_docs(query_tokens):\n",
    "    matching_docs=set()\n",
    "    token_list=list(inverted_index.keys())\n",
    "    \n",
    "    ### TODO:: use try catch\n",
    "    for tokens in query_tokens:\n",
    "        #print(\"token:\")\n",
    "        #print(tokens)\n",
    "        if (tokens in token_list):\n",
    "            x=inverted_index[tokens]\n",
    "            #print(\"containing docs:\")\n",
    "            #print(list(x.containing_documents.keys()))\n",
    "            documents=list(x.containing_documents.keys())\n",
    "            #print(documents)\n",
    "            #print(\"documents\")\n",
    "            #print(documents)\n",
    "            matching_docs.update(documents)\n",
    "    #print(matching_docs) #debug\n",
    "    return matching_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the query_norm (if needed)\n",
    "def query_norm(query_tokens):\n",
    "    #quertokens=documents_dictionary[document_id]\n",
    "    sum=0\n",
    "    term_freq=0\n",
    "    for token in set(query_tokens):\n",
    "        #print(term_freq)\n",
    "        term_freq=occurencies(query_tokens, token)\n",
    "        idfe=idf(token) #documents_dictionary is used to retrieve N\n",
    "        weight=term_freq*idfe\n",
    "        #print(weight) #debug\n",
    "        sum += weight * weight\n",
    "        #print(sum) #debug\n",
    "    return sum**(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranks the documents\n",
    "#returns an ordered list of tuples\n",
    "def rank_docs(similarity_dictionary):\n",
    "    from operator import itemgetter\n",
    "    sorted_docs = sorted(similarity_dictionary.items(), key=itemgetter(1), reverse=True)\n",
    "    \n",
    "    #debug\n",
    "    #print(\"sorted docs:\")\n",
    "    #print(sorted_docs)\n",
    "    #print(type(sorted_docs))\n",
    "    #print(sorted_docs[1])\n",
    "    \n",
    "    return sorted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_ranked_docs(query, smart):\n",
    "    #smart component:\n",
    "    #expand the query with semantically similar words\n",
    "    # we need the unprocessed words of the query\n",
    "    q = query\n",
    "    unprocessed_query_tokens = query.split()\n",
    "        \n",
    "    weigh_dictionary = {}\n",
    "    \n",
    "    #print(unprocessed_query_tokens)\n",
    "    \n",
    "    #if smart option == 1\n",
    "    #expand the query with model1\n",
    "    if (smart == 1):\n",
    "        for token in unprocessed_query_tokens:\n",
    "            #retrieve most similar words\n",
    "            try:\n",
    "                pairList = model.most_similar(positive=[token])\n",
    "                #for each similar word add th word in the query and\n",
    "                #an entry in the weight_dictionary to take into account the value of the similarity during the ranking phase\n",
    "                for pair in pairList:\n",
    "                    #consider only similar terms with similarity > 0.5\n",
    "                    if pair[1] > 0.6:\n",
    "                        q += \" \" + pair[0]\n",
    "                        weigh_dictionary[pair[0]] = pair[1]\n",
    "            except:\n",
    "                print(\"word: \" + token + \" not in the model vocaboulary\")\n",
    "                \n",
    "                \n",
    "    #if smart option == 2\n",
    "    #expand the query with model2\n",
    "    if (smart == 2):\n",
    "        for token in unprocessed_query_tokens:\n",
    "            #retrieve most similar words\n",
    "            try:\n",
    "                pairList = model2.most_similar(positive=[token])\n",
    "                #for each similar word add th word in the query and\n",
    "                #an entry in the weight_dictionary to take into account the value of the similarity during the ranking phase\n",
    "                for pair in pairList:\n",
    "                    #consider only similar terms with similarity > 0.5\n",
    "                    if pair[1] > 0.6:\n",
    "                        q += \" \" + pair[0]\n",
    "                        weigh_dictionary[pair[0]] = 0.9 * pair[1]\n",
    "            except:\n",
    "                print(\"word: \" + token + \" not in the model vocaboulary\")\n",
    "    \n",
    "    print(\"New expanded query: \")\n",
    "    print()\n",
    "    print('\"' + q + '\"')\n",
    "    print()\n",
    "    \n",
    "    query_tokens=preprocess(q)\n",
    "    #print(\"query tokens\")\n",
    "    #print(query_tokens)\n",
    "    \n",
    "    #query_weighted_norm=query_norm(query_tokens, documents_dictionary, inverted_index)\n",
    "    \n",
    "    #retrieve matching documents\n",
    "    matching_documents= retrive_docs(query_tokens)\n",
    "    print('Number of matching documents:')\n",
    "    print(len(matching_documents))\n",
    "    print()\n",
    "    print(\"ranking the documents...\")\n",
    "\n",
    "    #we pass the weigh_dictionary because \n",
    "    #the score for the expanded token has to be multipliaed \n",
    "    #for the similarity wrt the original token\n",
    "    similarity_dictionary=compute_similarity(query_tokens, matching_documents, weigh_dictionary)\n",
    "    \n",
    "    #print(ranking)\n",
    "    ranking=rank_docs(similarity_dictionary)\n",
    "    \n",
    "    #print(\"ranking\")\n",
    "    #print (ranking)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the url dictionary...\n"
     ]
    }
   ],
   "source": [
    "#load the dictionary documentId-> url from file\n",
    "#urlDictionary\n",
    "\n",
    "print(\"Loading the url dictionary...\")\n",
    "\n",
    "url_dictionary_path = \"url_dictionary2.txt\" #if needed change it\n",
    "\n",
    "with io.open(url_dictionary_path, 'r', encoding=\"utf-8\") as file:\n",
    "        temp_doc=file.read()\n",
    "        #parse the file\n",
    "        elemList = temp_doc.split(\"<element>\")\n",
    "        #print(elemList)\n",
    "        elemList= elemList[:-1]\n",
    "        #urlDictionary\n",
    "        urlDictionary = {}\n",
    "        for elem in elemList:\n",
    "            id = elem.split(\"<id|url>\")[0]\n",
    "            value = elem.split(\"<id|url>\")[1]\n",
    "            #print(\"id: \" + id)\n",
    "            #print(\"value: \" + value)\n",
    "            urlDictionary[id] = value\n",
    "            #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urlDictionary[\"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class token_class:\n",
    "    def __init__(self, idf=0,containing_documents= {}):\n",
    "        self.idf=idf #number of containing docs\n",
    "        self.containing_documents=containing_documents   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load inverted index from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract dictionary of containing document for the parsing of inverted_index\n",
    "def extract_dictionary (string):\n",
    "    containing_docs = {}\n",
    "    #remove '{' and '}'\n",
    "    string = string.strip()\n",
    "    string = string[1:-1]\n",
    "    #print(string)\n",
    "    ##split over ',' to extract pair document:tf\n",
    "    pairList = string.split(\",\")\n",
    "    # for pair in pairList\n",
    "    for pair in pairList:\n",
    "        #extract key(documentID) and value(frequency)\n",
    "        id= pair.split(\":\")[0]\n",
    "        #remove quotes sourrounding id\n",
    "        id = id.strip()\n",
    "        id = id[1:-1]\n",
    "        #print(id)\n",
    "        tf= pair.split(\":\")[1]\n",
    "        tf=tf.strip()\n",
    "        #print(tf)\n",
    "        #add an entry in the dictionary\n",
    "        containing_docs[id] = int(tf)\n",
    "\n",
    "        \n",
    "    #return the dictionary\n",
    "    return containing_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading inverted index..\n",
      "\n",
      "15% done\n",
      "\n",
      "31% done\n",
      "\n",
      "47% done\n",
      "\n",
      "63% done\n",
      "\n",
      "79% done\n",
      "\n",
      "95% done\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "#load inverted_index\n",
    "inverted_index = {}\n",
    "with io.open(\"inverted_index.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "        temp_doc=file.read()\n",
    "        #parse the file\n",
    "        elemList = temp_doc.split(\"</element>\")\n",
    "        #print(elemList)\n",
    "        elemList= elemList[:-1]\n",
    "        #weighted_norm dictionary\n",
    "        #weighted_norm_dictionary = {}\n",
    "        i = 0\n",
    "        print(\"loading inverted index..\")\n",
    "        print()\n",
    "        for elem in elemList:\n",
    "            ##extract the word\n",
    "            word = elem.split(\"<key|idf>\")[0]\n",
    "            ##etract the idf\n",
    "            idf = elem.split(\"<key|idf>\")[1]\n",
    "            idf = idf.split(\"<idf|cont_docs>\")[0]\n",
    "            #extract the dictionary of containing words\n",
    "            cont_docs = elem.split(\"<idf|cont_docs>\")[1]\n",
    "            #parse the dictionary\n",
    "            dictionary = extract_dictionary(cont_docs)\n",
    "            \n",
    "            #create an instance of the Class token and add it to inverted_index\n",
    "            key=word\n",
    "            value=token_class(idf, dictionary)\n",
    "            inverted_index[key]=value\n",
    "            i+=1\n",
    "            if (i%10000 == 0):\n",
    "                print(str(int(i*100/63000)) + \"% done\")\n",
    "                print()\n",
    "            \n",
    "            #print(i)\n",
    "            #print(\"word: \" + word)\n",
    "            #print(\"idf: \" + idf)\n",
    "            #print(\"cont_docs: \" + cont_docs)\n",
    "            #print()\n",
    "            #weighted_norm_dictionary[id] = float(value)\n",
    "            #print()'''\n",
    "        print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverted_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urlDictionary[\"2201\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the documents' norm from file...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "#load documents_weighted_norm from file\n",
    "print(\"Loading the documents' norm from file...\")\n",
    "with io.open(\"weighted_norm_dictionaryfull.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "        temp_doc=file.read()\n",
    "        #parse the file\n",
    "        elemList = temp_doc.split(\"<element>\")\n",
    "        elemList= elemList[:-1]\n",
    "        #print(elemList)\n",
    "        #weighted_norm dictionary\n",
    "        weighted_norm_dictionary = {}\n",
    "        for elem in elemList:\n",
    "            id = elem.split(\"<id|value>\")[0]\n",
    "            value = elem.split(\"<id|value>\")[1]\n",
    "            #print(\"id: \" + id)\n",
    "            #print(\"value: \" + value)\n",
    "            weighted_norm_dictionary[id] = float(value)\n",
    "            #print()\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weighted_norm_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_N_docs(ranked_docs, N):\n",
    "    return ranked_docs[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return a list of ranked documents for the query\n",
    "# query is the user input\n",
    "def retrieve(query, smart = 0):\n",
    "    q = userInput.get()\n",
    "    print(\"Input:\")\n",
    "    print()\n",
    "    print('\"' + q + ' \"\\n')\n",
    "    \n",
    "    result_query=retrive_ranked_docs(q, smart)\n",
    "    doc_list = map(operator.itemgetter(0),result_query)\n",
    "    ranking=list(doc_list)\n",
    "    print(\"Done!\")\n",
    "    showResults(ranking, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the occurencies of a word:String in a document: List of tokens\n",
    "def occurencies(List, String):\n",
    "    count=0\n",
    "    for x in List:\n",
    "        if x==String:\n",
    "            count=count +1\n",
    "    return count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(token, document_id):\n",
    "    #TODO:: use try catch\n",
    "    token_list=list(inverted_index.keys())\n",
    "    if not(token in token_list):\n",
    "        return 0\n",
    "    x=inverted_index[token]\n",
    "    #print(document_id)\n",
    "    keys=list(x.containing_documents.keys())\n",
    "    if document_id in keys:\n",
    "        tf=x.containing_documents[document_id] \n",
    "    else:\n",
    "        tf=0\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the idf of the token\n",
    "def idf(token): #document dictionary can be eliminated\n",
    "    N=len(urlDictionary) #total number of document in the collection\n",
    "    token_list=list(inverted_index.keys())\n",
    "    if not(token in token_list):\n",
    "        return 0\n",
    "    df=inverted_index[token].idf #number of containing docs\n",
    "    n=(N//float(df))\n",
    "    idf=math.log(n,2)\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query = \" hackathon\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rank =retrieve(query, 1)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Smart Component: GoogleNews-vectors-negative300 \n",
      "It could take a couple of minutes..\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pierv\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "###smart component\n",
    "print(\"Loading Smart Component: GoogleNews-vectors-negative300 \")\n",
    "print(\"It could take a couple of minutes..\")\n",
    "print()\n",
    "try:\n",
    "    from gensim.models import KeyedVectors\n",
    "    filename = 'GoogleNews-vectors-negative300.bin'\n",
    "    model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Coud not load global Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Smart Component: OwnWordToVecModel \n",
      "It could take a couple of minutes..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Smart Component: OwnWordToVecModel \")\n",
    "print(\"It could take a couple of minutes..\")\n",
    "print()\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "model2 = Word2Vec.load(\"ownModel.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from tkinter.ttk import *\n",
    "import webbrowser\n",
    "import subprocess\n",
    "class VerticalScrolledFrame(Frame):\n",
    "    \"\"\"A pure Tkinter scrollable frame that actually works!\n",
    "\n",
    "    * Use the 'interior' attribute to place widgets inside the scrollable frame\n",
    "    * Construct and pack/place/grid normally\n",
    "    * This frame only allows vertical scrolling\n",
    "    \"\"\"\n",
    "    def __init__(self, parent, *args, **kw):\n",
    "        Frame.__init__(self, parent, *args, **kw)            \n",
    "\n",
    "        # create a canvas object and a vertical scrollbar for scrolling it\n",
    "        vscrollbar = Scrollbar(self, orient=VERTICAL)\n",
    "        vscrollbar.pack(fill=Y, side=RIGHT, expand=FALSE)\n",
    "        canvas = Canvas(self, bd=0, highlightthickness=0,\n",
    "                        yscrollcommand=vscrollbar.set)\n",
    "        \n",
    "        canvas.pack(side=LEFT, fill=BOTH, expand=TRUE)\n",
    "        canvas.configure(background=\"LightBlue2\")\n",
    "        vscrollbar.config(command=canvas.yview)\n",
    "\n",
    "        # reset the view\n",
    "        canvas.xview_moveto(0)\n",
    "        canvas.yview_moveto(0)\n",
    "\n",
    "        # create a frame inside the canvas which will be scrolled with it\n",
    "        self.interior = interior = Frame(canvas)\n",
    "        interior_id = canvas.create_window(0, 0, window=interior,\n",
    "                                           anchor=NW)\n",
    "\n",
    "        # track changes to the canvas and frame width and sync them,\n",
    "        # also updating the scrollbar\n",
    "        def _configure_interior(event):\n",
    "            # update the scrollbars to match the size of the inner frame\n",
    "            size = (interior.winfo_reqwidth(), interior.winfo_reqheight())\n",
    "            canvas.config(scrollregion=\"0 0 %s %s\" % size)\n",
    "            if interior.winfo_reqwidth() != canvas.winfo_width():\n",
    "                # update the canvas's width to fit the inner frame\n",
    "                canvas.config(width=interior.winfo_reqwidth())\n",
    "\n",
    "        interior.bind('<Configure>', _configure_interior)\n",
    "\n",
    "        def _configure_canvas(event):\n",
    "            if interior.winfo_reqwidth() != canvas.winfo_width():\n",
    "                # update the inner frame's width to fill the canvas\n",
    "                canvas.itemconfigure(interior_id, width=canvas.winfo_width())\n",
    "        canvas.bind('<Configure>', _configure_canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OpenUrl(url):\n",
    "    webbrowser.open_new(url)\n",
    "    \n",
    "def showMoreRes(index,button, scframe, rank):\n",
    "    end = False\n",
    "    for i in range(index,index+10):\n",
    "        button.append(Button(scframe.interior, text=urlDictionary[str(rank[i])], command=lambda aurl=urlDictionary[str(rank[i])]:OpenUrl(aurl),  bg=\"DeepSkyBlue2\"))\n",
    "        button[i].pack()\n",
    "        if (i>= (len(rank) -2)):\n",
    "            endButton = Button(scframe.interior,text=\"no more results to show\", bg=\"light goldenrod\")\n",
    "            endButton.pack()\n",
    "            end = True\n",
    "            break;\n",
    "        \n",
    "    counter = index + 10\n",
    "    \n",
    "    if not(end):\n",
    "        showMore = Button(scframe.interior,text=\"show more results\",command= lambda cont=counter:showMoreRes(counter,button, scframe, rank),  bg=\"light goldenrod\")\n",
    "        showMore.pack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showResults(rank, input):\n",
    "    results = Tk()\n",
    "    results.title(\"Results for: \" + input)\n",
    "    results.configure(background=\"LightBlue2\")\n",
    "\n",
    "    scframe = VerticalScrolledFrame(results)\n",
    "    scframe.pack()\n",
    "    \n",
    "    button = []\n",
    "    counter = 0\n",
    "    for i in range(10):\n",
    "        if (i>= (len(rank) -1)):\n",
    "            endButton = Button(scframe.interior,text=\"no more results to show\", bg=\"light goldenrod\")\n",
    "            endButton.pack()\n",
    "            break;\n",
    "        button.append(Button(scframe.interior, text=urlDictionary[str(rank[i])], command=lambda rurl=urlDictionary[str(rank[i])]:OpenUrl(rurl),  bg=\"DeepSkyBlue2\"))\n",
    "        button[i].pack()\n",
    "        counter += 1\n",
    "        \n",
    "        if(i==9):\n",
    "            showMore = Button(scframe.interior,text=\"show more results\",command= lambda cont=counter:showMoreRes(counter,button, scframe,rank),  bg=\"light goldenrod\")\n",
    "            showMore.pack()\n",
    "    mainloop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "\n",
      "\"hackathon \"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pierv\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New expanded query: \n",
      "\n",
      "\"hackathon Hackathon iPhoneDevCamp hackathons RHoK hack_thon barcamp unconference\"\n",
      "\n",
      "Number of matching documents:\n",
      "56\n",
      "\n",
      "ranking the documents...\n",
      "Done!\n",
      "Input:\n",
      "\n",
      "\"hackathon \"\n",
      "\n",
      "New expanded query: \n",
      "\n",
      "\"hackathon fax arrow proctoring map admissions health vision home st disabilities\"\n",
      "\n",
      "Number of matching documents:\n",
      "2804\n",
      "\n",
      "ranking the documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pierv\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "import subprocess\n",
    "\n",
    "\n",
    "\n",
    "gui = Tk()\n",
    "gui.title('Search Engine')\n",
    "gui.configure(background=\"LightBlue2\")\n",
    "Label(gui, text='Query Input:',  bg=\"DeepSkyBlue2\").grid(row=2)\n",
    "\n",
    "userInput = Entry(gui) \n",
    "userInput.grid(row=2, column=1)\n",
    "\n",
    "messageLabel = Label(gui, text = \"Hello! Please insert a query.\",  bg=\"light goldenrod\").grid(row=0)\n",
    "\n",
    "searchButton = Button(gui,command=lambda input=userInput.get():retrieve(input,0),text=\"Search\", bg=\"DeepSkyBlue2\")\n",
    "searchButton.grid(row=3,column=0)\n",
    "searchSmartButton1 = Button(gui,command=lambda input=userInput.get():retrieve(input,1),text=\"Search globally Smart\", bg=\"DeepSkyBlue2\")\n",
    "searchSmartButton1.grid(row=3,column=1)\n",
    "searchSmartButton2 = Button(gui,command=lambda input=userInput.get():retrieve(input,2),text=\"Search locally Smart\", bg=\"DeepSkyBlue2\")\n",
    "searchSmartButton2.grid(row=3,column=2)\n",
    "mainloop() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
