{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this notebook we train a WordToVec model  on the crawled documents and explore which words are retrieved as similar w.r.t. a given word: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory_path ='./documents'\n",
    "documents=os.listdir(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model vocabulary...\n",
      "0.0% done.\n",
      "3.12% done.\n",
      "6.25% done.\n",
      "9.38% done.\n",
      "12.5% done.\n",
      "15.62% done.\n",
      "18.75% done.\n",
      "21.88% done.\n",
      "25.0% done.\n",
      "28.12% done.\n",
      "31.25% done.\n",
      "34.38% done.\n",
      "37.5% done.\n",
      "40.62% done.\n",
      "43.75% done.\n",
      "46.88% done.\n",
      "50.0% done.\n",
      "53.12% done.\n",
      "56.25% done.\n",
      "59.38% done.\n",
      "62.5% done.\n",
      "65.62% done.\n",
      "68.75% done.\n",
      "71.88% done.\n",
      "75.0% done.\n",
      "78.12% done.\n",
      "81.25% done.\n",
      "84.38% done.\n",
      "87.5% done.\n",
      "90.62% done.\n",
      "93.75% done.\n",
      "96.88% done.\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#let's create a vocabulary from all the documents we crawled\n",
    "vocabulary = \"\"\n",
    "print(\"Building the model vocabulary...\")\n",
    "i = 0\n",
    "for document in documents:\n",
    "    if (i % 100 == 0):\n",
    "        print(str(round((i*100/len(documents)),2)) + \"% done.\" )\n",
    "    i +=1\n",
    "    \n",
    "    with io.open(directory_path +'/'+ document, 'r', encoding=\"utf-8\") as file:\n",
    "        temp_doc=file.read()\n",
    "        vocabulary += temp_doc\n",
    "        vocabulary += \" \"\n",
    "        vocabulary += \"\\n\"\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pierv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaing the text\n",
    "processed_article = vocabulary.lower()  \n",
    "processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )  \n",
    "processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "\n",
    "# Preparing the dataset\n",
    "all_sentences = nltk.sent_tokenize(processed_article)\n",
    "\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4734231"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "### relative path of the file\n",
    "stopwords_path =\"./stopwords.txt\"  \n",
    "stopwords = open(stopwords_path).read()\n",
    "stopwords_list=stopwords.split()\n",
    "for stop in stopwords_list:\n",
    "    temp = stemmer.stem(stop)\n",
    "    if not (temp in stopwords_list):\n",
    "    \n",
    "       stopwords_list.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords  \n",
    "for i in range(len(all_words)):  \n",
    "    all_words[i] = [w for w in all_words[i] if w not in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3042221"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train the model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(all_words, min_count=2, window = 3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocabulary = word2vec.wv.vocab  \n",
    "print(vocabulary)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pierv\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "sim_words = word2vec.wv.most_similar('student')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('digital', 0.9986699819564819),\n",
       " ('accessibility', 0.9978376626968384),\n",
       " ('services', 0.9978151321411133),\n",
       " ('complete', 0.9977531433105469),\n",
       " ('campus', 0.9976484775543213),\n",
       " ('uic', 0.9975920915603638),\n",
       " ('search', 0.997422993183136),\n",
       " ('form', 0.9974071383476257),\n",
       " ('drc', 0.9972301125526428),\n",
       " ('exam', 0.9972104430198669)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "word2vec.save(\"ownModel.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
